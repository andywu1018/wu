# -*- coding: utf-8 -*-
import codecs
from sklearn.feature_extraction.text import TfidfTransformer
from sklearn.naive_bayes import MultinomialNB
from sklearn.feature_extraction.text import CountVectorizer
from sklearn import preprocessing
import os
from sklearn.externals import joblib
from sklearn.cross_validation import train_test_split
from sklearn.cross_validation import StratifiedKFold
from sklearn.cross_validation import KFold
from sklearn.metrics import precision_recall_curve
from sklearn.metrics import classification_report
from sklearn.metrics import confusion_matrix,classification_report
from sklearn.feature_selection import SelectKBest, chi2
import pandas as pd
import jieba
import re
import string
import time
import numpy as np

delEStr = string.punctuation + ' ' + string.digits
filename_list = []
category_list = []
words_list = []
all_words = {}
rootpath = 'D:\文本分类\训练数据'
category = os.listdir(rootpath)


#======读取编码为utf8的本文文件=====
def readtext(path):
    text = codecs.open(path, 'r', encoding='utf-8')
    content = text.read()
    text.close()
    return content
stopwords = readtext('stop_words.txt')

#======分词========

def filewordProcess(content):
    wordlist = []
    content = re.sub(r'\s+',' ',content)
    content = re.sub(r'\n',' ',content)
    content = re.sub(r'\t',' ',content)
    for seg in jieba.cut(content,cut_all=False):
        if seg not in stopwords:
            if seg != ' ':
                wordlist.append(seg)
    file_content = ' '.join(wordlist)
    return file_content

for categoryName in category:
    if (categoryName == '.DS_Store'): continue
    categoryPath = os.path.join(rootpath, categoryName)  # 这个类别的路径
    filesList = os.listdir(categoryPath)  # 这个类别内所有文件列表
    # 循环对每个文件分词
    for filename in filesList:
        if (filename == '.DS_Store'): continue
        starttime = time.clock()
        contents = open(os.path.join(categoryPath, filename), encoding='utf8').read()
        wordProcessed = filewordProcess(contents)  # 内容分词成列表
        # 暂时不做#filenameWordProcessed = fileWordProcess(filename) # 文件名分词，单独做特征
        #         words_list.append((wordProcessed,categoryName,filename)) # 训练集格式：[(当前文件内词列表，类别，文件名)]
        words_list.append(wordProcessed)
        filename_list.append(filename)
        category_list.append(categoryName)
        endtime = time.clock();
        print('类别:%s >>>>文件:%s >>>>导入用时: %.3f' % (categoryName, filename, endtime - starttime))


def Normalization(x):
    x[x > 1] = 1
    return x



# 创建词向量矩阵，创建tfidf值矩阵
def tfidf_mat(words_list,filename_list,category_list):
    freWord = CountVectorizer(stop_words='english')
    transformer = TfidfTransformer(use_idf=True)
    fre_matrix = freWord.fit_transform(words_list)
    tfidf = transformer.fit_transform(fre_matrix)

    feature_names = freWord.get_feature_names()  # 特征名
    freWordVector_df = pd.DataFrame(fre_matrix.toarray())  # 全词库 词频 向量矩阵
    tfidf_df = pd.DataFrame(tfidf.toarray())  # tfidf值矩阵
    print(tfidf_df)
    # print freWordVector_df
    # tf-idf 筛选
    tfidf_sx_featuresindex = tfidf_df.sum(axis=0).sort_values(ascending=False)[:10000].index
    freWord_tfsx_df = freWordVector_df.ix[:, tfidf_sx_featuresindex]  # tfidf法筛选后的词向量矩阵
    df_columns = pd.Series(feature_names)[tfidf_sx_featuresindex]
    print(df_columns)
    print(freWord_tfsx_df)

    tfidf_df_1 = freWord_tfsx_df#.apply(Normalization)
    tfidf_df_1.columns = df_columns
    le = preprocessing.LabelEncoder()  # 评分到0到n_class-1
    tfidf_df_1['label'] = le.fit_transform(category_list)
    tfidf_df_1.index = filename_list
    return tfidf_df_1,df_columns

# 卡方检验
def Chi_square_test(tfidf_df_1):
    ch2 = SelectKBest(chi2, k=10000)
    nolabel_feature = [x for x in tfidf_df_1.columns if x not in ['label']]
    ch2_sx_np = ch2.fit_transform(tfidf_df_1[nolabel_feature],tfidf_df_1['label'])
    label_np = np.array(tfidf_df_1['label'])
    X = ch2_sx_np
    y = label_np
    skf = KFold(len(X)+len(y),n_folds=3)
    y_pre = y.copy()
    for train_index,test_index in skf:
        X_train,X_test = X[train_index],X[test_index]
        y_train,y_test = y[train_index],y[test_index]
        clf = MultinomialNB().fit(X_train, y_train)
        y_pre[test_index] = clf.predict(X_test)
    print('准确率为 %.6f' % (np.mean(y_pre == y)))
    return y,y_pre,nolabel_feature


