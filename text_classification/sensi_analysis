# -*- coding: utf-8 -*-
"""
Created on Thu May 18 16:32:08 2017

@author: admin
"""
import pandas as pd
import MySQLdb
import numpy as np
import datetime
import jieba
from collections import Counter
from gensim.models.doc2vec import TaggedDocument,Doc2Vec
from gensim import models,corpora,similarities
#----------------------情感词典------------------------  
                     
data = pd.read_table('emotion.txt',encoding='utf-8') #/Emotion_analysis_dictionary/emotion.txt linux下文件所在位置
posdict = []
negdict = []
posemotion = []
negemotion = []
possentiments = []
negsentiments = []
for i in range(len(data)):
    if data[data.columns[6]][i] == 1:
        posdict.append(data[data.columns[0]][i])
        posemotion.append(data[data.columns[5]][i])
        possentiments.append(data[data.columns[4]][i])
    if data[data.columns[6]][i] == 2:
        negdict.append(data[data.columns[0]][i])
        negemotion.append(data[data.columns[5]][i])
        negsentiments.append(data[data.columns[4]][i])
#==============================加载=============================        
def read_lines(filename):
	fp = open(filename, 'r')
	lines = []
	for line in fp.readlines():
		line = line.strip()
		line = line.decode("utf-8")
		lines.append(line)
	fp.close()
	return lines
#==============================去词=============================
def del_stopwords(seg_sent):
	stopwords = read_lines("stop_words.txt")                             #-----> 读取停用词表
	new_sent = []  
	for word in seg_sent:
		if word in stopwords:
			continue
		else:
			new_sent.append(word)
	return new_sent
#==============================分词=============================
def segmentation(sentence):
	seg_list = jieba.cut(sentence)
	seg_result = []
	for w in seg_list:
		seg_result.append(w)
	return seg_result
#==============================分句=============================
def cut_sentence(words):
#	words = words.decode('utf8')
	start = 0
	i = 0
	token = 'meaningless'
	sents = []
	punt_list = ',.!?;~，。！？；～… '.decode('utf8')
	for word in words:
		if word not in punt_list:   # 如果不是标点符号
			i += 1
			token = list(words[start:i+2]).pop()
		elif word in punt_list and token in punt_list:  # 处理省略号
			i += 1
			token = list(words[start:i+2]).pop()
		else:
			sents.append(words[start:i+1])   # 断句
			start = i + 1
			i += 1
	if start < len(words):   # 处理最后的部分
		sents.append(words[start:])
	return sents
#========================程度副词处理===========================
mostdict = read_lines('most.txt')   
verydict = read_lines('very.txt')   
moredict = read_lines('more.txt')  
ishdict = read_lines('ish.txt')   
insufficientdict = read_lines('insufficiently.txt')  
inversedict = read_lines('inverse.txt')
#-------------------------------------------------------
def match(word, sentiment_value):
    if word in mostdict:
        sentiment_value *= 2.0
    elif word in verydict:
        sentiment_value *= 1.75
    elif word in moredict:
        sentiment_value *= 1.5
    elif word in ishdict:
        sentiment_value *= 1.2
    elif word in insufficientdict:
        sentiment_value *= 0.5
    elif word in inversedict:
        sentiment_value *= -1
    return sentiment_value
#=================情感得分的最后处理，防止出现负数===================
def transform_to_positive_num(poscount, negcount):
    pos_count = 0
    neg_count = 0
    if poscount < 0 and negcount >= 0:
        neg_count += negcount - poscount
        pos_count = 0
    elif negcount < 0 and poscount >= 0:
        pos_count = poscount - negcount
        neg_count = 0
    elif poscount < 0 and negcount < 0:
        neg_count = -poscount
        pos_count = -negcount
    else:
        pos_count = poscount
        neg_count = negcount
    return (pos_count, neg_count)
#=====================求单条新闻的情感倾向总得分===================
def single_review_sentiment_score(news_sent):
    single_review_senti_score = []
    possen = []
    negsen = []
    cuted_review = cut_sentence(news_sent)  
    for sent in cuted_review:
        seg_sent = segmentation(sent)   # 分词
        seg_sent = del_stopwords(seg_sent)[:]
        i = 0    # 记录扫描到的词的位置
        s = 0    # 记录情感词的位置
        poscount = 0    # 记录该分句中的积极情感得分
        negcount = 0    # 记录该分句中的消极情感得分
        for word in seg_sent:   # 逐词分析                
            if word in posdict:  # 如果是积极情感词
                poscount += posemotion[posdict.index(word)]   # 积极得分+1
                possen.append(possentiments[posdict.index(word)])
                for w in seg_sent[s:i]:
                    if w in negdict:
                        negcount += 2*negemotion[negdict.index(w)]
                    poscount = match(w, poscount)
                s = i + 1   # 记录情感词的位置变化

            elif word in negdict:  # 如果是消极情感词
                negcount += negemotion[negdict.index(word)]
                negsen.append(negsentiments[negdict.index(word)])
                for w in seg_sent[s:i]:
                    negcount = match(w, negcount)
                s = i + 1
            elif word == "！".decode("utf-8") or word == "!".decode('utf-8'):
                for w2 in seg_sent[::-1]:  # 倒序扫描感叹号前的情感词，发现后权值+2，然后退出循环
                    if w2 in posdict:
                        poscount += 2*posemotion[posdict.index(w2)]
                        break
                    elif w2 in negdict:
                        negcount += 2*negemotion[negdict.index(w2)]
                        break
            i += 1
        single_review_senti_score.append(transform_to_positive_num(poscount, negcount))   # 对得分做最后处理
    pos_result, neg_result = 0, 0   # 分别记录积极情感总得分和消极情感总得分
    for res1, res2 in single_review_senti_score:  # 每个分句循环累加
        pos_result += res1
        neg_result += res2
    posC = Counter(possen)
    negC = Counter(negsen)   
    happy = posC['PA'] + posC['PE']
    fine = posC['PD'] + posC['PH'] + posC['PG'] + posC['PB'] + posC['PH']
    fury = negC['NA']
    wane = negC['NB'] + negC['NJ'] + negC['NH'] +negC['PF']
    fear = negC['NI'] + negC['NC'] + negC['NG']
    vice = negC['NE'] + negC['ND'] + negC['NN'] + negC['NK'] + negC['NL']
    shock = negC['PC']
    result = pos_result - neg_result   # 该条新闻情感的最终得分
    result = round(result, 1)
    return result,pos_result,neg_result,happy,fine,fury,wane,fear,vice,shock

#=======================================文本相似度计算=============================================
def Text_similarity(title,content,IDS,Sources,Dates):
    Sous = []
    corpora_documents = []
    for item_text in content:
        item_seg = list(jieba.cut(item_text))     #分词处理
        corpora_documents.append(item_seg)
    dictionary = corpora.Dictionary(corpora_documents) # 生成字典和向量语料
    corpus = [dictionary.doc2bow(text) for text in corpora_documents]
    tfidf_model = models.TfidfModel(corpus)
    corpus_tfidf = tfidf_model[corpus]
    lsi = models.LsiModel(corpus_tfidf)  
    corpus_lsi = lsi[corpus_tfidf]  
    similarity_lsi=similarities.Similarity('Similarity-LSI-index', corpus_lsi, num_features=500,num_best=5) 
    for i in range(len(content)):
        if len(content[i]) > 0:
            test_data_1 = content[i]
            test_cut_raw_1 = list(jieba.cut(test_data_1))  
            test_corpus_1 = dictionary.doc2bow(test_cut_raw_1)  
            test_corpus_tfidf_1=tfidf_model[test_corpus_1]
            test_corpus_lsi_1 = lsi[test_corpus_tfidf_1] 
            sourc = []
            for j in range(len(similarity_lsi[test_corpus_lsi_1])):
                if similarity_lsi[test_corpus_lsi_1][j][1] >= 0.90 and similarity_lsi[test_corpus_lsi_1][j][0] != i: # >=1.0 严格去重复
                    if similarity_lsi[test_corpus_lsi_1][j][0] > i:
                        content[similarity_lsi[test_corpus_lsi_1][j][0]] = []
                        sourc.append(Sources[similarity_lsi[test_corpus_lsi_1][j][0]])
            sourc.append(Sources[i])            
            Sous.append(sourc)
    Content = []        
    Title = []
    ID_NEWS = []
    DATES = []
    for i in range(len(content)):
        if len(content[i]) > 0:
            Content.append(content[i])
            Title.append(title[i])
            ID_NEWS.append(IDS[i])
            DATES.append(Dates[i])                       
    return Title,Content,ID_NEWS,Sous,DATES

#=======================================清洗数据=============================================
def Information_cleaning(ID,titles,contexts,source,dates,timepoint):
    title = []
    content = []
    IDS = []
    Source = []
    Dates = []
    for i in range(len(ID)):
        if datetime.datetime.strptime(str(dates[i])[0:10], "%Y-%m-%d") > datetime.datetime.strptime(timepoint, "%Y-%m-%d"): #判断时效性
            title.append(titles[i])
            content.append(contexts[i])
            IDS.append(ID[i])
            Source.append(sources[i])
            Dates.append(str(dates[i]))
    return title,content,IDS,Source,Dates


import matplotlib.pyplot as plt 
from pylab import mpl
mpl.rcParams['font.sans-serif'] = ['SimHei']
        
if __name__ == '__main__': 
    conn= MySQLdb.connect(
            host='60.191.74.66',
            port = 3306,
            user='lwj',
            passwd='123456',
            db ='china_news',
            charset='utf8'
            )
    table_names = "zhejiang_content"
    sqlcmd = "SELECT * FROM " + table_names
    data = pd.read_sql(sqlcmd,conn)
    ID = data[data.columns[0]]
    sources = data[data.columns[10]]
    titles = data[data.columns[1]]
    contexts = data[data.columns[3]] 
    dates =  data[data.columns[4]]
    timepoint = str(datetime.datetime.now() - datetime.timedelta(days=30))[0:10]
    #title,content,IDS,Sources,Dates = Information_cleaning(ID,titles,contexts,sources,dates,timepoint)
    #Title,Content,ID_NEWS,Sous,DATES = Text_similarity(title,content,IDS,Sources,Dates)
    Title,Content,ID_NEWS,Sous,DATES = Information_cleaning(ID,titles,contexts,sources,dates,timepoint)
    print len(Content)
    SentimentsCount = []
    for i in range(len(Content)):
        print len(Content) - i
        result,pos_result, neg_result,happy,fine,fury,wane,fear,vice,shock = single_review_sentiment_score(Content[i])
        SentimentsCount.append([result,pos_result, neg_result,happy,fine,fury,wane,fear,vice,shock])

    Sensum = np.array(SentimentsCount)
    Senmean = Sensum.sum(0)/len(SentimentsCount)
    labels1 = ['正面'.decode('utf-8'),'负面'.decode('utf-8')]
    labels2 = ['乐'.decode('utf-8'),'好'.decode('utf-8'),'怒'.decode('utf-8'),'衰'.decode('utf-8'),'惧'.decode('utf-8'),'恶'.decode('utf-8'),'惊'.decode('utf-8')]
    colors1 = ['yellowgreen','lightskyblue']
    colors2 = ['yellowgreen','lightskyblue','red','gold', 'lightskyblue','darkslategray','cyan']
    explode1 = Senmean[1:3]
    explode2 = Senmean[3::]
    plt.figure()
    plt.subplot(121)
    plt.pie(explode2,labels=labels2,colors=colors2,autopct='%1.2f%%')
    plt.title("情绪分析".decode('utf-8'))
    plt.subplot(122)
    plt.pie(explode1,labels=labels1,colors=colors1,autopct='%1.2f%%')
    plt.title("情感分析".decode('utf-8'))



