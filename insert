# -*- coding: utf-8 -*-
"""
Created on Wed Jun 28 15:49:56 2017
实时监测数据库中有无新ID出现，若出现新ID，将其内容进行分类，并将分类结果返回数据库
@author: wwl
"""
import os  
import pandas as pd
import jieba
import numpy as np
import re
import string
import MySQLdb
import matplotlib.pyplot as plt
from sklearn.externals import joblib
from sklearn.feature_extraction.text import TfidfTransformer  
from sklearn.feature_extraction.text import CountVectorizer  
from sklearn.naive_bayes import MultinomialNB  
from sklearn.cross_validation import train_test_split   
from sklearn.cross_validation import StratifiedKFold  
from sklearn.cross_validation import KFold  
from sklearn.metrics import precision_recall_curve    
from sklearn.metrics import classification_report 
from sklearn.metrics import confusion_matrix,classification_report 
from sklearn.feature_selection import SelectKBest, chi2 
from sklearn import preprocessing
import datetime
import pickle
import time


#==============================加载=============================        
def read_lines(filename):
	fp = open(filename, 'r')
	lines = []
	for line in fp.readlines():
		line = line.strip()
		line = line.decode("utf-8")
		lines.append(line)
	fp.close()
	return lines

stopwords = read_lines("D:/stop_words.txt") 
#==============================================================
def fileWordProcess(contents,stopwords):  
    wordsList = []
    contents = re.sub(r'\[0-9]*','',contents)
    contents = re.sub(r'\s+',' ',contents) # trans 多空格 to 空格  
    contents = re.sub(r'\n',' ',contents)  # trans 换行 to 空格  
    contents = re.sub(r'\t',' ',contents)  # trans Tab to 空格
    contents = re.sub(r'https:\/\/+.*',' ',contents)#去除url
    contents = re.sub(r'&+[a-zA-Z]*',' ',contents)
    
    for seg in jieba.cut(contents):  
        #seg = seg.encode('utf8')          #转化为utf-8后，文件格式不统一，下面的判断是无效的
        if seg not in stopwords:           # remove 停用词  
            if seg!=' ':                   # remove 空格  
                wordsList.append(seg)      # create 文件词列表  
    file_string = ' '.join(wordsList)              
    return file_string


#===========================创建词向量矩阵，创建tfidf值矩阵，预测用================

def normalization(x):
    x[x>1]=1  
    return x


def tfidf_mat_pre(words_list,df_columns):
    freWord = CountVectorizer(stop_words='english')  
    transformer = TfidfTransformer()  
    fre_matrix = freWord.fit_transform(words_list)  
    tfidf = transformer.fit_transform(fre_matrix)
    feature_names = freWord.get_feature_names()           #
    tfidf_df = pd.DataFrame(np.zeros((1,20000)),index=['未知'.decode('utf-8')],columns=range(20000))
    tfidf_df.columns=df_columns
    le = preprocessing.LabelEncoder()
    tfidf_df['label'] = le.fit_transform(['未知'.decode('utf-8')])
    for strr in feature_names:
        for i in range(len(tfidf_df.columns)):
            if strr == tfidf_df.columns[i]:
                tfidf_df[tfidf_df.columns[i]] = 1
    ch2 = SelectKBest(chi2, k=10000)
    nolabel_feature = [x for x in tfidf_df.columns if x not in ['label']]      
    ch2_sx_np = ch2.fit_transform(tfidf_df[nolabel_feature],tfidf_df['label'])
    return ch2_sx_np

#=============================返回3个特征位置和预测值=============================
def get_top_3(pre_list):
    first=pre_list.argsort()[0,len(pre_list[0,:])-1]
    second=pre_list.argsort()[0,len(pre_list[0,:])-2]
    third=pre_list.argsort()[0,len(pre_list[0,:])-3]
    first_of_list=pre_list[0,first]
    second_of_list=pre_list[0,second]
    third_of_list=pre_list[0,third]
    return first,second,third,first_of_list,second_of_list,third_of_list

if __name__ == '__main__':    
    conn = MySQLdb.connect(
            host='127.0.0.1',
            port = 3306,
            user='root',
            passwd='root',
            db ='mysql',
            charset='utf8'
            )
    table_names = "科技部新闻"
    sqlcmd = "SELECT * FROM " + table_names
    data = pd.read_sql(sqlcmd,conn)
    
    #sources = data[data.columns[9]]
    #classer = data[data.columns[1]]
    #contexts = data['内容'] 
    #classers = []
    category_list=['仓储物流','信贷融资','农业服务业','医疗健康','大数据','扶贫',
    '政策','新能源','星创天地','林业服务业','法规','渔业服务业','电子商务','畜牧服务业'
    ,'社交','科技成果','科技特派员','证券投资','财政公开','通知公告','金融资讯']
    df_columns=pickle.load(open('D:/pickle/df_columns.txt','r'))
    ID_last=6990
    #classfier=''
    while True:
        ID = data[data.columns[0]]
        if ID_last == ID.max():
            time.sleep(5)
            print 'ID_last==ID.max()'
        else:
            data_new=data[ID_last:ID.max()]
            contents=data_new['内容']
            for i in range(len(contents)):
                try:
                
                    seg_sent = fileWordProcess(contents[i+ID_last],stopwords)
                    words_list = [seg_sent]
                    tfidf_df=tfidf_mat_pre(words_list,df_columns)
                    clf = joblib.load('D:/work/model/normal_extratree100.m')
                    print clf.predict(tfidf_df)
                    pre_list=clf.predict_proba(tfidf_df)
                    first,second,third,first_of_list,second_of_list,third_of_list=get_top_3(pre_list)
                    if first_of_list<0.1:
                        classfier='其他'
                    if first_of_list >0.1:
                        if first_of_list/second_of_list>2:
                            classfier='%s'%(category_list[first])
                        if first_of_list/second_of_list<2:
                            if first_of_list/second_of_list<2:
                                classfier='%s,%s,%s'%(category_list[first],category_list[second],category_list[third])
                            else:
                                classfier='%s,%s'%(category_list[first],category_list[second])
                    
                    print classfier
                    insert_id=ID_last+1+i                
                    cur = conn.cursor()
                    cur.execute("update 科技部新闻 set 分类结果 = '%s' where id ='%d'"%(classfier,insert_id))
                    cur.close() 
                except:
                    print 'meet error'                               
            ID_last=ID.max()
        
    










