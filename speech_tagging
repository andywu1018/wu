# -*- coding: utf-8 -*-
"""
Created on Sat May 27 15:14:46 2017

@author: admin
"""
import pandas as pd
import numpy as np
import pynlpir
import MySQLdb
import datetime
import jieba
from collections import Counter
from gensim.models.doc2vec import TaggedDocument,Doc2Vec
from gensim import models,corpora,similarities
pynlpir.open()


def segmentation(text):
    word_noun = {}
    word_verb = {}
    word_adjective = {}
    words= pynlpir.segment(text)
    for ws,pos in words:
        if pos == 'noun':
            if ws not in word_noun:
                if len(ws)>=2:
                    word_noun[ws] = 1
            else:
                if len(ws)>=2:
                    word_noun[ws] += 1
        if pos == 'verb':
            if ws not in word_verb:
                if len(ws)>=2:
                    word_verb[ws] = 1
            else:
                if len(ws)>=2:
                    word_verb[ws] += 1
        if pos == 'adjective':
            if ws not in word_adjective:
                if len(ws)>=2:
                    word_adjective[ws] = 1
            else:
                if len(ws)>=2:
                    word_adjective[ws] += 1
    #<----------------------词频统计---------------------->                              
    word_noun = sorted(word_noun.iteritems(),key=lambda asd:asd[1],reverse=True) #词频降序
    word_verb = sorted(word_verb.iteritems(),key=lambda asd:asd[1],reverse=True) #词频降序
    word_adjective = sorted(word_adjective.iteritems(),key=lambda asd:asd[1],reverse=True) #词频降序
    return word_noun,word_verb,word_adjective


def dictionary_extraction(text):
    key_words = pynlpir.get_key_words(text)
    words = pynlpir.segment(text,pos_names='child')
    key_word = []
    personal_name = []
    organization = []
    toponym = []
    time_word = []
    for ws,pos in words:
        if pos == 'personal name' and ws not in personal_name:
            personal_name.append(ws)
        if pos == 'organization/group name' and ws not in organization:
            organization.append(ws)
        if pos == 'toponym' and ws not in toponym:
            toponym.append(ws)
        if pos == 'time word' and ws not in time_word:
            time_word.append(ws)
    if len(key_words) > 8:        
        key_word = key_words[0:8]
    if len(personal_name) > 8:        
        personal_name = personal_name[0:8]
    if len(organization) > 8:        
        organization = organization[0:8]
    if len(toponym) > 8:        
        toponym = toponym[0:8]
    if len(toponym) > 8:        
        time_word = time_word[0:8]    
    return key_word,personal_name,organization,toponym,time_word


def createtree(key_word,personal_name,organization,toponym,time_word,timepoint):
    kws = {}
    pns = {}
    org = {}
    topo = {}
    tws = {}
    for ws in key_word:
        if ws not in kws:
            kws[ws] = 1
        else:
            kws[ws] += 1
    for ws in personal_name:
        if ws not in pns:
            pns[ws] = 1
        else:
            pns[ws] += 1
    for ws in organization:
        if ws not in org:
            org[ws] = 1
        else:
            org[ws] += 1
    for ws in toponym:
        if ws not in topo:
            topo[ws] = 1
        else:
            topo[ws] += 1
    for ws in time_word:
        if ws not in tws:
            tws[ws] = 1
        else:
            tws[ws] += 1
               
    kws = sorted(kws.iteritems(),key=lambda asd:asd[1],reverse=True) #词频降序
    pns = sorted(pns.iteritems(),key=lambda asd:asd[1],reverse=True) #词频降序
    org = sorted(org.iteritems(),key=lambda asd:asd[1],reverse=True) #词频降序
    topo = sorted(topo.iteritems(),key=lambda asd:asd[1],reverse=True) #词频降序
    tws = sorted(tws.iteritems(),key=lambda asd:asd[1],reverse=True) #词频降序  
    key_words = []
    personal_names = []
    toponyms = []
    organizations = []
    time_words = []
    for i in range(min([10,len(kws)])):
         key_words.append(kws[i][0])
    for i in range(min([10,len(pns)])):     
         personal_names.append(pns[i][0])
    for i in range(min([10,len(topo)])):     
         toponyms.append(topo[i][0])
    for i in range(min([10,len(org)])):     
         organizations.append(org[i][0])
    for i in range(min([10,len(tws)])):     
         time_words.append(tws[i][0])
    
    title = timepoint + '至今发布新闻综合分析'.decode('utf-8')                        
    mytree = {}
    mytree.setdefault(title,{}) 
    mytree[title].setdefault('关键词'.decode('utf-8'),key_words)
    mytree[title].setdefault('人名'.decode('utf-8'),personal_names)
    mytree[title].setdefault('地名'.decode('utf-8'),toponyms)
    mytree[title].setdefault('机构'.decode('utf-8'),organizations)
    mytree[title].setdefault('时间'.decode('utf-8'),time_words)
    return mytree

#—————————————————————————————————————————————————————————————————
                          # 决策树绘图
#—————————————————————————————————————————————————————————————————

import matplotlib.pyplot as plt 
from pylab import mpl
mpl.rcParams['font.sans-serif'] = ['SimHei'] 

    
def plotTree(mytree):
    treedeep = []
    plt.figure()
    ax = plt.subplot(111)
    for k,dicts in mytree.items():
        if k not in treedeep:
            treedeep.append(k)
        for key,values in dicts.items():
            if key not in treedeep:
                treedeep.append(key)
                
    theta = np.arange(0,2*np.pi,2*np.pi/(len(treedeep)-1))
    x = 2000*np.cos(theta)
    y = 2000*np.sin(theta)
    plt.plot([np.zeros(len(treedeep)-1),x],[np.zeros(len(treedeep)-1),y])
    plt.text(0,0,treedeep[0],bbox=dict(facecolor='orange', boxstyle='round,pad=1'))
    for i in range(len(treedeep)-1):
        plt.text(x[i],y[i],treedeep[i+1],bbox=dict(facecolor='red', boxstyle='round,pad=1'))
    idd = 0    
    for k,dicts in mytree.items():
        for key,values in dicts.items():
            if len(values) > 0:
                theta = np.arange(0,2*np.pi,2*np.pi/(len(values)))
                x1 = x[idd]+1000*np.cos(theta)
                y1 = y[idd]+1000*np.sin(theta)      
                plt.plot([x[idd]*np.ones(len(values)),x1],[y[idd]*np.ones(len(values)),y1])
                idd += 1
                for i in range(len(values)):
                    plt.text(x1[i],y1[i],values[i],bbox=dict(facecolor='yellow', boxstyle='round,pad=1')) 
            else:
                idd += 1
    ax.set_xticks([])
    ax.set_yticks([])
    #figstring = tit + '.jpg'                
    #plt.savefig(figstring)
        
#=======================================文本相似度计算=============================================
def Text_similarity(title,content,IDS,Sources,Dates):
    Sous = []
    corpora_documents = []
    for item_text in content:
        item_seg = list(jieba.cut(item_text))     #分词处理
        corpora_documents.append(item_seg)
    dictionary = corpora.Dictionary(corpora_documents) # 生成字典和向量语料
    corpus = [dictionary.doc2bow(text) for text in corpora_documents]
    tfidf_model = models.TfidfModel(corpus)
    corpus_tfidf = tfidf_model[corpus]
    lsi = models.LsiModel(corpus_tfidf)  
    corpus_lsi = lsi[corpus_tfidf]  
    similarity_lsi=similarities.Similarity('Similarity-LSI-index', corpus_lsi, num_features=500,num_best=5) 
    for i in range(len(content)):
        if len(content[i]) > 0:
            test_data_1 = content[i]
            test_cut_raw_1 = list(jieba.cut(test_data_1))  
            test_corpus_1 = dictionary.doc2bow(test_cut_raw_1)  
            test_corpus_tfidf_1=tfidf_model[test_corpus_1]
            test_corpus_lsi_1 = lsi[test_corpus_tfidf_1] 
            sourc = []
            for j in range(len(similarity_lsi[test_corpus_lsi_1])):
                if similarity_lsi[test_corpus_lsi_1][j][1] >= 0.90 and similarity_lsi[test_corpus_lsi_1][j][0] != i: # >=1.0 严格去重复
                    if similarity_lsi[test_corpus_lsi_1][j][0] > i:
                        content[similarity_lsi[test_corpus_lsi_1][j][0]] = []
                        sourc.append(Sources[similarity_lsi[test_corpus_lsi_1][j][0]])
            sourc.append(Sources[i])            
            Sous.append(sourc)
    Content = []        
    Title = []
    ID_NEWS = []
    DATES = []
    for i in range(len(content)):
        if len(content[i]) > 0:
            Content.append(content[i])
            Title.append(title[i])
            ID_NEWS.append(IDS[i])
            DATES.append(Dates[i])
            
            
    return Title,Content,ID_NEWS,Sous,DATES

#=======================================筛选数据=============================================
def Information_cleaning(ID,titles,contexts,source,dates,timepoint):
    title = []
    content = []
    IDS = []
    Source = []
    Dates = []
    for i in range(len(ID)):
        if datetime.datetime.strptime(str(dates[i])[0:10], "%Y-%m-%d") > datetime.datetime.strptime(timepoint, "%Y-%m-%d"): #判断时效性
            title.append(titles[i])
            content.append(contexts[i])
            IDS.append(ID[i])
            Source.append(sources[i])
            Dates.append(str(dates[i]))
    return title,content,IDS,Source,Dates    
    
 
    
#=====================求单条新闻的情感倾向总得分===================



if __name__ == '__main__': 
    conn= MySQLdb.connect(
            host='60.191.74.66',
            port = 3306,
            user='lwj',
            passwd='123456',
            db ='china_news',
            charset='utf8'
            )
    table_names = "china_content"
    sqlcmd = "SELECT * FROM " + table_names
    data = pd.read_sql(sqlcmd,conn)
    ID = data[data.columns[0]]
    sources = data[data.columns[9]]
    titles = data[data.columns[1]]
    contexts = data[data.columns[3]] 
    dates =  data[data.columns[4]]
    timepoint = str(datetime.datetime.now() - datetime.timedelta(days=30))[0:10]
    title,content,IDS,Sources,Dates = Information_cleaning(ID,titles,contexts,sources,dates,timepoint)
    Title,Content,ID_NEWS,Sous,DATES = Text_similarity(title,content,IDS,Sources,Dates)    
    WORD_NOUN = []
    WORD_VERB = []
    WORD_ADJECTIVE = []
    KEY_WORD = []
    PERSONAL_NAME = []
    ORGANIZATION = []
    TOPONYM = []
    TIME_WORD = []
    for i in range(len(Content)):
        word_noun,word_verb,word_adjective = segmentation(Content[i])
        WORD_NOUN.extend(word_noun)
        WORD_VERB.extend(word_verb)
        WORD_ADJECTIVE.extend(word_adjective)
        key_word,personal_name,organization,toponym,time_word = dictionary_extraction(Content[i])
        KEY_WORD.extend(key_word)
        PERSONAL_NAME.extend(personal_name)
        ORGANIZATION.extend(organization)
        TOPONYM.extend(toponym)
        TIME_WORD.extend(time_word)
    tree = createtree(KEY_WORD,PERSONAL_NAME,ORGANIZATION,TOPONYM,TIME_WORD,timepoint)
    plotTree(tree)  
    


    plt.figure()
    plt.subplot(131)
    label = []
    vales = []
    WN = sorted(dict(WORD_NOUN).iteritems(),key=lambda t:t[1],reverse=True)
    for i in range(10):
        label.append(WN[i][0])
        vales.append(WN[i][1])
    x = vales
    idx = np.arange(len(x)) 
    color = 'yellowgreen' 
    plt.barh(idx, x, color=color) 
    plt.yticks(idx,label) 
    plt.grid(axis='x')
    plt.subplot(132)
    label = []
    vales = []
    WV = sorted(dict(WORD_VERB).iteritems(),key=lambda t:t[1],reverse=True)
    for i in range(10):
        label.append(WV[i][0])
        vales.append(WV[i][1])
    x = vales
    idx = np.arange(len(x)) 
    color = 'yellowgreen' 
    plt.barh(idx, x, color=color) 
    plt.yticks(idx,label) 
    plt.grid(axis='x') 
    plt.subplot(133)
    label = []
    vales = []
    WA = sorted(dict(WORD_ADJECTIVE).iteritems(),key=lambda t:t[1],reverse=True)
    for i in range(10):
        label.append(WA[i][0])
        vales.append(WA[i][1])
    x = vales
    idx = np.arange(len(x)) 
    color = 'yellowgreen' 
    plt.barh(idx, x, color=color) 
    plt.yticks(idx,label) 
    plt.grid(axis='x') 
    plt.show()
    pynlpir.close()





    
    
    
    
    
    
    
    
    
    
    
